{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad71c3a8-bc41-47c2-bfaa-4757cbceedf2",
   "metadata": {},
   "source": [
    "# Practicum: Modern Hopfield Network Spell Checker\n",
    "In a lecture, an idea popped into my head in passing: \"Could we use a modern Hopfield Network to do spell checking and word recommendation?\" Well, you are in luck! In this practicum, we will implement a modern Hopfield Network and do a proof-of-concept test of its capabilities for these tasks. \n",
    "\n",
    "* __Hypothesis__: Suppose we have words embedded in some low-dimensional space, i.e., we have the vector representation of each word in a large text corpus. Then, we should be able to load these vectors into a modern Hofpfield network (which memorizes vectors), and let its self-attention-based update mechanism find the correct true word given a corrupted version of that word.\n",
    "\n",
    "Thus, the modern Hopfield network could serve as a spell checker that returns a correctly spelled word given a misspelled variation of that word.\n",
    "\n",
    "## Tasks\n",
    "Before we get started, execute the `Run All Cells` command to check if you have any code or setup issues. Code issues, post a question on EdDiscussion.\n",
    "* __Task 1: Setup, Data, Constants__: In this task, we set up the computational environment by including [the `Include.jl` file](Include.jl), loading any needed resources, such as sample datasets, and setting up any required constants.\n",
    "*  __Task 2: Can we recover an uncorrupted memory?__ In this task, we'll create a modern Hopfield network model, load the test data, and then check if we can recover an _uncorrupted_ memory (word) from the network. \n",
    "* __Task 3: Retrieve a corrupted memory from the network__: In this task, we'll repeat the process above, but this time we'll start from a corrupted memory. We'll do this by cutting off a fraction of the word and then see if the model recovers the correct memory given the corrupted starting memory. \n",
    "\n",
    "Let's get started! (Don't forget to answer the discussion questions!)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e0a48",
   "metadata": {},
   "source": [
    "## Background\n",
    "A modern Hopfield network addresses many of the perceived limitations of the original Hopfield network. The original Hopfield network was limited to binary values and could only store a limited number of patterns. The modern Hopfield network uses continuous values and can store a large number of patterns.\n",
    " \n",
    "* We'll used the following paper to guide our implementation and analysis: [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "* In addition, for a detailed discussion of the key milestones in the development of modern Hopfield networks, check out [Hopfield Networks is All You Need Blog, GitHub.io](https://ml-jku.github.io/hopfield-layers/).\n",
    "\n",
    "### Algorithm\n",
    "The user provides a set of memory vectors $\\mathbf{X} = \\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{m}\\right\\}$, where $\\mathbf{x}_{i} \\in \\mathbb{R}^{n}$ is a memory vector of size $n$ and $m$ is the number of memory vectors. Further, the user provides an initial _partial memory_ $\\mathbf{s}_{\\circ} \\in \\mathbb{R}^{n}$, which is a vector of size $n$ that is a partial version of one of the memory vectors and specifies the _temperature_ $\\beta$ of the system.\n",
    "\n",
    "__Initialize__ the network with the memory vectors $\\mathbf{X}$, and the inverse temperature $\\beta$. Set current state to the initial state $\\mathbf{s} \\gets \\mathbf{s}_{\\circ}$\n",
    "\n",
    " - Until convergence __do__:\n",
    "      1. Compute the _current_ probability vector defined as $\\mathbf{p} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s})$ where $\\mathbf{s}$ is the _current_ state vector, and $\\mathbf{X}^{\\top}$ is the transpose of the memory matrix $\\mathbf{X}$.\n",
    "      2. Compute the _next_ state vector $\\mathbf{s}^{\\prime} = \\mathbf{X}\\mathbf{p}$ and the _next_ probability vector $\\mathbf{p}^{\\prime} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s}^{\\prime})$.\n",
    "      3. If $\\mathbf{p}^{\\prime}$ is _close_ to $\\mathbf{p}$ or we run out of iterations, then __stop__. For example, $\\lVert \\mathbf{p}^{\\prime} - \\mathbf{p}\\rVert_{2}^{2} \\leq \\epsilon$ for some small $\\epsilon > 0$.\n",
    "      4. Otherwise, update the state $\\mathbf{s} \\gets\\mathbf{s}^{\\prime}$, and __go back to__ step 1.\n",
    "- End __do__ loop\n",
    "   \n",
    "This algorithm is implemented in [the `recover(...)` method](src/Compute.jl) provided in [the `src/Compute.jl` file](src/Compute.jl) of this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b935753",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "In this task, we set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184b2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\"); # load a bunch of libs, including the ones we need to work with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aba20e",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Before we load the data, let's set up some constants that we will use in the exercise. Please enter the number of words (memories) you want your Hopefield network to memorize, the embedding dimension and the inverse temperature $\\beta$.\n",
    "\n",
    "* The `number_of_words_to_memorize::Int` should be less than or equal to the number of words in the dataset that we load below.\n",
    "* The `number_of_embedding_dimesions::Int` depends upon the pretrained embedding dataset that we will use. In this case, we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) dataset, with `50` embedding dimensions.\n",
    "* The inverse temperature $\\beta > 0$ is a hyperparameter that controls the sharpness of the softmax distribution. A higher value of $\\beta$ will make the distribution sharper, while a lower value will make it smoother. Set an initial value of `1.5` for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f255b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words_to_memorize = 2^5; # TODO: Enter how many memories we want to memorize (Int ≥ 0)\n",
    "number_of_embedding_dimesions = 50; # TODO: Enter the number of embedding dimensions for each word (Int = 50)\n",
    "β = 1.5; # TODO: Enter an inverse temperature of the system (high T -> low β) Float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cc6c7",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this section, let's load a vector embedding from words. We'll use the [GloVe pretrained word embedding dataset](https://nlp.stanford.edu/projects/glove/) as the memories for our Hopfield model. The **GloVe (Global Vectors for Word Representation)** dataset is a widely used pre-trained word embedding resource developed at Stanford University by Pennington, Socher, and Manning. \n",
    "* _What is it?_ It constructs vector representations of words by aggregating global word co-occurrence statistics from a corpus, enabling semantic relationships to be captured in vector space. GloVe embeddings have been trained on large datasets such as Wikipedia, Gigaword, and Common Crawl, offering dimensionalities typically ranging from 50 to 300. These embeddings are foundational for many NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
    "* See: [Pennington et al., EMNLP 2014, GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) for more details on this dataset.\n",
    "\n",
    "__IMPORTANT READ THIS!!__ The word embedding dataset is too large to check into the repository. Instead, you'll __must__ download it from [here](\"https://drive.google.com/file/d/1tP9W4R1Ap7vp2AAmgoVEJ5lMfQAk5Fym/view?usp=share_link\"). Once the embedding dataset is downloaded, put it into the `data` directory of this repository, and then run the code block below to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34627ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, vec2word = let\n",
    "\n",
    "    # Do we have the embeddings downloaded?\n",
    "    data = nothing;\n",
    "    if (isfile(joinpath(_PATH_TO_DATA, \"glove_6B_50d.jld2\")) == false) \n",
    "        throw(\"Oooops! The glove_6B_50d.jld2 file was not found in the /data directory. Did you download and move it into the /data directory?\")\n",
    "    end\n",
    "    data = JLD2.load(joinpath(_PATH_TO_DATA, \"glove_6B_50d.jld2\"));\n",
    "\n",
    "    # load the embeddings\n",
    "    word2vec = data[\"word2vec\"] # this is a Dict{String, NTuple{50,Float64}}: word -> embedding (50d)\n",
    "    vec2word = data[\"vec2word\"] # this is a Dict{NTuple{50, Float64}, String} embedding (50d) -> word\n",
    "\n",
    "    # return -\n",
    "    (word2vec, vec2word)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221159d",
   "metadata": {},
   "source": [
    "__Test data__: Now that we have loaded the [pretrained GloVe dataset](https://nlp.stanford.edu/projects/glove/), let's select a (random) subset of length `number_of_words_to_memorize` from the dataset to encode into the modern Hopfield network. \n",
    "* This code block returns the `test_words::Array{String,1}` array of words that we will use to test the Hopfield network. The embedding vectors corresponding to these words are stored in the `test_vocabulary::Array{Float64,2}` array, where the vector representation of each word is stored in the columns of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c94374",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words, test_vocabulary = let\n",
    "\n",
    "    # initialize -\n",
    "    vocabulary = Array{Float64,2}(undef, number_of_words_to_memorize, number_of_embedding_dimesions); # this is a matrix of Float64\n",
    "    test_words = Array{String,1}(undef, number_of_words_to_memorize); # this is a vector of strings\n",
    "    total_number_of_words = length(word2vec); # this is the total number of words in the dataset\n",
    "    index_of_words_to_learn = randperm(total_number_of_words)[1:number_of_words_to_memorize]; # this is the random index of words to learn\n",
    "\n",
    "    # get the keys of word2vec -\n",
    "    words = keys(word2vec) |> collect; # this is a vector of strings\n",
    "\n",
    "    # loop over the words to learn\n",
    "    for i ∈ eachindex(index_of_words_to_learn)\n",
    "        \n",
    "        j = index_of_words_to_learn[i]; # this is the index of the word we want to learn\n",
    "        wⱼ = words[j]; # this is the word we want to learn\n",
    "        test_words[i] = wⱼ; # this is the word we want to learn \n",
    "        embedding = word2vec[wⱼ]; # this is the embedding of the word we want to learn\n",
    "\n",
    "        for j ∈ 1:number_of_embedding_dimesions\n",
    "            vocabulary[i,j] = embedding[j]; # this is the embedding of the word we want to learn\n",
    "        end\n",
    "    end\n",
    "\n",
    "    test_vocabulary = vocabulary |> transpose |> Matrix; # this is the vocabulary we want to learn\n",
    "\n",
    "    # return -\n",
    "    (test_words, test_vocabulary);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8722efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Vector{String}:\n",
       " \"américo\"\n",
       " \"histologic\"\n",
       " \"2in\"\n",
       " \"rhadamanthus\"\n",
       " \"abū\"\n",
       " \"transcorp\"\n",
       " \"jayewardene\"\n",
       " \"brassicas\"\n",
       " \"webshop\"\n",
       " \"harf\"\n",
       " \"faysal\"\n",
       " \"milićević\"\n",
       " \"3,540\"\n",
       " ⋮\n",
       " \"alveolus\"\n",
       " \"trade-related\"\n",
       " \"washout\"\n",
       " \"14-footer\"\n",
       " \"coss\"\n",
       " \"perugia\"\n",
       " \"8,180\"\n",
       " \"especialista\"\n",
       " \"finalisation\"\n",
       " \"reproductively\"\n",
       " \"evdokia\"\n",
       " \"lakhi\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words # What are our words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa383c44",
   "metadata": {},
   "source": [
    "__Check__: Let's check that the `test_words` and `test_vocabulary` arrays are of the correct size. \n",
    "* The `test_words` array should be of size `number_of_words_to_memorize`. The `test_vocabulary` array should be of size `number_of_embedding_dimensions` $\\times$ `number_of_words_to_memorize`, i.e., the memorized words should be on the columns of the `test_vocabulary` array.\n",
    "* We'll use the [@assert macro](hhttps://docs.julialang.org/en/v1/base/base/#Base.@assert) to check that the arrays are of the correct size. If the assertion fails, an error will be raised and the program will stop. If no error is raised, the program will continue (everything is correctly sized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc729cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    @assert length(test_words) == number_of_words_to_memorize;\n",
    "    @assert size(test_vocabulary, 1) == number_of_embedding_dimesions;\n",
    "    @assert size(test_vocabulary, 2) == number_of_words_to_memorize;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ddd93",
   "metadata": {},
   "source": [
    "## Task 2: Can we recover an uncorrupted memory?\n",
    "In this task, we'll create a modern Hopfield network model, load the test data, and then check if we can recover an _uncorrupted_ memory from the network. We'll do this by starting from a state vector $\\mathbf{s}_{\\circ}$ that is a column from the data loaded into the model.\n",
    "\n",
    "* __Expectation__: The network will converge to a local minimum, but we are not guaranteed that the local minimum corresponds to the original _uncorrupted_ memory. Given an _uncorrupted_ memory as a starting point, we expect to recover that _uncorrupted_ memory with a small number of mistakes when the system temperature is cold, i.e., $\\beta > \\beta^{\\star}$, where $\\beta^{\\star}$ is a (unknown) threshold temperature that (potentially) depends on the number of memories, and other factors. As the temperature increases, we expect the error probability to increase. \n",
    " \n",
    "Let's start by creating a model of a modern Hopfield network. \n",
    "* We'll construct [a `MyModernHopfieldNetworkModel` instance](src/Types.jl) using a custom [`build(...)` function](src/Factory.jl). The [`build(...)` method](src/Factory.jl) takes the type of thing we want to build, the (linearized) image library we want to encode, and the (inverse) system temperature $\\beta$ as inputs — images along the columns.\n",
    "* The [`build(...)` function](src/Factory.jl) returns a `MyModernHopfieldNetworkModel` instance, where the image library is stored in the `X::Array{Float64,2}` field, and the system temperature is stored in the `β::Float64` field.\n",
    "\n",
    "We'll store the Hopfield network instance in the `mymodel::MyModernHopfieldNetworkModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0b88f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = let\n",
    "\n",
    "    # initialize -\n",
    "    model = nothing; # this is the model we want to build\n",
    "    memorycollection =test_vocabulary; # words (memories) on columns\n",
    "    index_vector = 1:number_of_words_to_memorize |> collect; # this is the index of the words we want to learn\n",
    "    words = keys(test_vocabulary) |> collect; # this is a vector of strings\n",
    "    \n",
    "    # build model -\n",
    "    model = build(MyModernHopfieldNetworkModel, (\n",
    "            memories = memorycollection, # this is the data we want to memorize. Images on columns\n",
    "            β = β, # Inverse temperature of the system. A big beta means we are more likely to get the right answer\n",
    "    ));\n",
    "\n",
    "    model; # return the model to the calling scope\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b18b75",
   "metadata": {},
   "source": [
    "__Check__: Let's do a quick check to make sure we are doing what we think we are doing when we loaded the memories into the model. The columns of the `model.X` field should be the words that we are encoding into the Hopfield network. Thus, we should be able to grab a column from `model.X` and look it up in the original `vec2word` dictionary.\n",
    "* We'll use the [@assert macro](hhttps://docs.julialang.org/en/v1/base/base/#Base.@assert) to check that the true word, and the word encoded in the model are the same. If the assertion fails, an error will be raised and the program will stop. If no error is raised, the program will continue (everything is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8905f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    \n",
    "    # initialize -\n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    index_to_check = rand(1:number_of_words_to_memorize); # what index do we want to check? (random)\n",
    "    \n",
    "    # Get the true word, and the word we think we learned -\n",
    "    eᵢ = X[:,index_to_check] |> Tuple # this is the embedding of the word we want to learn\n",
    "    wᵢ = test_words[index_to_check]; # this is the word we want to learn\n",
    "    ŵᵢ = vec2word[eᵢ]; # this is the word we think we learned\n",
    "\n",
    "    # Compare the two words -\n",
    "    @assert wᵢ == ŵᵢ; # this is the word we want to learn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cf809",
   "metadata": {},
   "source": [
    "__Retrieve a memory from the network__: Next, we'll test if we can recover uncorrupted and corrupted memories from the Hopfield network.\n",
    "Let's start by specifying which memory we are trying to recover in the `memoryindextorecover::Int` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e93d68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryindextorecover = 21; # TODO: Specify which memory vector will we choose (must be between 1 and number_of_words_to_memorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193276",
   "metadata": {},
   "source": [
    "Next, let's build an uncorrupted and corrupted initial condition vector using the true word emebedding vector. We'll store \n",
    "the uncorrupted word in the `sₒ::Array{Float64,1}` variable, while the corrupted word will be stored in the `s₁::Array{Float64,1}` variable. Let's start with the uncorrupted memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5184ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sₒ = mymodel.X[:,memoryindextorecover]; # this is the memory vector we want to recover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92773242",
   "metadata": {},
   "source": [
    "What word does `memoryindextorecover::Int64` point to? Let's do a quick check to make sure $\\mathbf{s}_{\\circ}$ is what we think it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7067c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word at index 21 that we (think) we encoded is: alveolus. Check: the true word is: alveolus\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    p = β*(transpose(X) * sₒ) |> s-> NNlib.softmax(s) # this is the probability of the word we want to learn\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    w = test_words[memoryindextorecover]; # this is the word we want to learn\n",
    "    println(\"The word at index $(memoryindextorecover) that we (think) we encoded is: $(ŵ). Check: the true word is: \", w);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558716a",
   "metadata": {},
   "source": [
    "Now that we have a starting memory encoded in the state vector $\\mathbf{s}_{\\circ}$, can we recover the original uncorrupted word? We are guaranteed a word, but maybe _not_ the correct one.\n",
    "* __Implementation__: We implemented the modern Hopfield recovery algorithm above in [the `recover(...)` method](src/Compute.jl). This method takes our `model::MyModernHopfieldNetworkModel` instance, the initial configuration vector `sₒ::Array{Float64,1}`, the maximum number `maxiterations::Int64`, and an iteration tolerance parameter `ϵ::Float64`. This method will continue to iterate until the probability vector converges, or we run out of iterations.\n",
    "* [The `recover(...)` method](src/Compute.jl) returns the recovered word vector in the `ŝₒ::Array{Float32,1}` variable, the word vectors at each iteration are stored in the `fₒ::Dict{Int, Array{Float64,2}}` dictionary, and the probability of the words at each iteration in the `pₒ::Dict{Int, Array{Float64,2}}` variable. The dictionaries are indexed from `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3049a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ŝₒ,fₒ,pₒ) = recover(mymodel, sₒ, maxiterations = 10000, ϵ = 1e-16); # iterate until we hit stop condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b18c41",
   "metadata": {},
   "source": [
    "How many iterations did it take to converge? (this will be the length, i.e.,. the numbr of keys of the `fₒ` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d6ef339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 5\n"
     ]
    }
   ],
   "source": [
    "println(\"How many iterations: $(length(fₒ))\") # how many iterations did we need to converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a404ed",
   "metadata": {},
   "source": [
    "__Which word did we recover?__ We can check if the recovered word is what we expected by looking at the probability of the recovered words stored in the `pₒ::Dict{Int, Array{Float64,2}}` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29818778",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_word_uncorrupted = let \n",
    "    \n",
    "    # initialize -\n",
    "    number_of_iterations = length(fₒ); # how many iterations did we need to converge? \n",
    "    p = pₒ[number_of_iterations - 1]; # this is the probability of the word we want to learn  (0 based)\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    \n",
    "    ŵ; # return the word we *think* we learned\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f273e9-63fb-44a9-bf93-8ca0817babb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recovered_word_uncorrupted = alveolus\n"
     ]
    }
   ],
   "source": [
    "println(\"The recovered_word_uncorrupted = $(recovered_word_uncorrupted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8309f2",
   "metadata": {},
   "source": [
    "__Check__: Let's check to see if the recovered word is identical to the original word (not guaranteed). We can do this by checking the `s₁::Array{Float32,1}` variable against the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13cbb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    true_word = test_words[memoryindextorecover]; # this is the word we want to learn\n",
    "    @assert recovered_word_uncorrupted == true_word\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea75fdb2-91ab-4007-a947-23d803a08793",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "1. We hypothesized that the network's retrieval error frequency was directly proportional to the system temperature; i.e., as the system gets hotter (smaller $\\beta$), we should see more mistakes (the `@assert` statement above should fail when the network makes a mistake).\n",
    "    - Run the Task 2 logic a few times for different values of $\\beta$, i.e., $\\beta = 1.5$ (base), $\\beta = 1.0$, and $\\beta = 0.15$. Does the network behave like we expect (is our intuition consistent with what you see)?\n",
    "    - Explain what you think is going on with the role of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b4d23f-e431-4b2d-b25b-3972c3ab903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Put DQ answer here -- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade2bc6d-e39f-4243-8be5-176bfa1bd727",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ1 = false; # TODO: update the flag value {true | false} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdd8dd",
   "metadata": {},
   "source": [
    "## Task 3: Retrieve a corrupted memory from the network\n",
    "In this task, we'll repeat the process above, but this time we'll start from a corrupted memory. We'll do this by cutting off a fraction of the word and then see if the model recovers the correct memory given the corrupted starting point. \n",
    "\n",
    "Let's get started by building a corrupted memory. We'll iterate through each embedding dimension from the uncorrupted word; sometimes, we'll make a mistake and replace the correct embedding value with an incorrect value. We'll control how oftern we make a mistake using the hyperparameter $\\theta$.\n",
    "* _What is the $\\theta$ parameter?_ The $\\theta$ hyperparameter controls how often we make mistakes. Its interpretation depends upon our _mistake_ model. For example, if we are cutting off some fraction of the embedding dimension, then $\\theta$ describes the fraction of the image we are cutting off. \n",
    "\n",
    "Whichever mistake model we use, the $\\theta\\in[0,1]$. We store the corrupted word in the `s₁::Array{Float64,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b286495",
   "metadata": {},
   "outputs": [],
   "source": [
    "s₁ = let\n",
    "\n",
    "    # initialize -\n",
    "    sₒ = mymodel.X[:,memoryindextorecover]; # this is the memory vector we want to recover (uncorrupted)\n",
    "    s₁ = Array{Float32,1}(undef, number_of_embedding_dimesions); # initialize some space to store the corrupted word\n",
    "    θ = 0.40; # TODO: set a mistake threshold (1 - θ is the fraction the original memory that we retain)\n",
    "\n",
    "    # Corruption model: Cutoff part of the memory\n",
    "    cutoff = (1-θ)*number_of_embedding_dimesions |> x-> round(Int,x);\n",
    "    for i ∈ 1:number_of_embedding_dimesions\n",
    "        eᵢ =  sₒ[i]; # We have some gray-scale values in the original vector, need to perturb\n",
    "        if (i ≤ cutoff)\n",
    "            s₁[i] = eᵢ;\n",
    "        else\n",
    "            s₁[i] = β*randn(); # add some random noise (proportional to β)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    s₁ # return corrupted data to the calling scope\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455bfb7",
   "metadata": {},
   "source": [
    "__What is the closest word to the corrupted word__? Using the self-attention mechanism as our similarity measure, we can compute the most probable memory given the corrupted memory. `Unhide` the code block below, to see how we computed and printed a table holding the probability of observing `number_of_top_words::Int` words using [the `pretty_table(...)` method exported by the `PrettyTables.jl` package](https://github.com/ronisbr/PrettyTables.jl).\n",
    "* _How should we interpret this table_? Think of this table as the network's first guess. When we mutate a word, we are directly changing the embedding. Thus, there is no guarantee that the mutated word will be in our vocabulary (highly unlikely given this type of mutation model). However, we can use the self-attention mechanism of the modern Hopfield model to compute the probability that the mutated word corresponds to a word in our vocabulary. Pretty neat! That's what we have in the table.\n",
    "\n",
    "Depending upon the $\\theta$ parameter, the correct word may have only a small probability of being closest word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6567f49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ ======= ==============\n",
      " \u001b[1m      word \u001b[0m \u001b[1m index \u001b[0m \u001b[1m probability \u001b[0m\n",
      " \u001b[90m    String \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m     Float64 \u001b[0m\n",
      "============ ======= ==============\n",
      "   alveolus      21       0.99789\n",
      "     cimade      15    0.00104567\n",
      "       cjnt      14   0.000240341\n",
      "  brassicas       8   0.000170976\n",
      "     repute      16    0.00016555\n",
      "============ ======= ==============\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    number_of_top_words = 5; # TODO: You can see how many words are the closest to the corrupted memory by changing this number\n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    p = β*(transpose(X) * s₁) |> s-> NNlib.softmax(s) # this is the probability of the word we want to learn\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "\n",
    "    # make a table -\n",
    "    df = DataFrame();\n",
    "    sorted_indices = sortperm(p, rev=true); # sort the indices of the probabilities\n",
    "    for i ∈ 1:number_of_top_words\n",
    "        index = sorted_indices[i]; # this is the index of the word we think we learned\n",
    "        ŵ = test_words[index]; # this is the word we think we learned\n",
    "        p̂ = p[index]; # this is the probability of the word we think we learned\n",
    "        push!(df, (word=ŵ, index = index, probability=p̂)); # add the word and its probability to the table\n",
    "    end\n",
    "    pretty_table(df, tf = tf_simple)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18945b96",
   "metadata": {},
   "source": [
    "__Do we converge to the correct word starting from a corrupted word__? Now that we have a starting (corrupted) memory encoded in the $\\mathbf{s}_{1}$ state vector, can we recover the original uncorrupted memory, i.e., the uncorrupted word? We are guaranteed to converge to a word, but maybe _not_ the correct one.\n",
    "* __Implementation__: We implemented the modern Hopfield recovery algorithm above in [the `recover(...)` method](src/Compute.jl). This method takes our `model::MyModernHopfieldNetworkModel` instance, the initial configuration vector `s₁::Array{Float64,1}`, the maximum number `maxiterations::Int64`, and the iteration tolerance parameter `ϵ::Float64`. \n",
    "* [The `recover(...)` method](src/Compute.jl) returns the recovered image in the `ŝ₁::Array{Float64,1}` variable, the word embeddings at each iteration in the `f₁::Dict{Int, Array{Float64,2}}` dictionary, and the probability of the image at each iteration in the `p₁::Dict{Int, Array{Float64,2}}` variable. The dictionaries are indexed from `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99768f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ŝ₁,f₁,p₁) = recover(mymodel, s₁, maxiterations = 10000, ϵ = 1e-16); # iterate until we hit stop condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d370391",
   "metadata": {},
   "source": [
    "How many iterations did it take to converge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dac18af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network converged to an answer (staring from a corrupted word) in: 6 iterations.\n"
     ]
    }
   ],
   "source": [
    "println(\"The network converged to an answer (staring from a corrupted word) in: $(length(f₁)) iterations.\") # how many iterations did we need to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4ee0b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"alveolus\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recovered_word_corrupted = let \n",
    "    \n",
    "    # initialize -\n",
    "    number_of_iterations = length(f₁); # how many iterations did we need to converge? \n",
    "    p = p₁[number_of_iterations - 1]; # this is the probability of the word we want to learn  (0 based)\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    \n",
    "    ŵ; # return the word we *think* we recovered\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31bbe2-e74c-42a3-8b77-081df09e7df7",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "1. Depending upon the $\\theta$ parameter, the original word and the system inverse temperature $\\beta$, sketch when belive the we _should_ recover the corrupted word.\n",
    "   - Run the Task 3 logic on a few samples with different combinations of the uncorrected word, a range of values for the $\\theta$ parameter, and the system inverse temperature $\\beta$, and sketch when you think is going on. We expect to be able to get the correct word as long as the starting input is not _too_ corrupted, and the system temperature is not _too_ hot.\n",
    "   - Can you confirm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7e559fe-ef48-4c0c-a7fe-e03ca11310ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- DQ answer goes here -- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ed12a39-b415-45f0-94ca-5e18484624e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "did_I_answer_DQ2 = false; # TODO: update the flag value {true | false} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390089b2-99b3-4b50-aa85-70b8e5f7cd78",
   "metadata": {},
   "source": [
    "## Fun (totally optional) directions we could go with this idea in the future\n",
    "This was a proof-of-concept exploration of modern Hopfield networks in a simple text application, namely a spell checker. In addition to more deeply exploring the probability of mistakes, the role of $\\beta$, etc., we could go a zillion different directions. Here are a few ideas below.\n",
    "\n",
    "### Maybe different embeddings?\n",
    "While combining GloVe word embeddings with a modern Hopfield network is a promising approach for spell checking, there are several important caveats to consider (that we could explore):\n",
    "* __Out-of-Vocabulary (OOV) Issues__: GloVe is a word-level embedding model that only provides vectors for words in its training corpus. If a misspelled word does not exist in the GloVe vocabulary, it will lack a pre-trained embedding, making it impossible to map the misspelling into the embedding space directly. This is a critical limitation for handling typos, especially those that significantly distort the word.\n",
    "* __Lack of Subword Information__: Unlike other embeddings, GloVe embeddings do not incorporate subword (character n-gram) information. GloVe is less robust to spelling variations that retain obvious subword patterns. As a result, misspelled words that share common roots or morphological components with correct words won't necessarily be mapped close together in the embedding space.\n",
    "* __Semantic Rather Than Orthographic Proximity__: GloVe embeddings are designed to capture semantic similarity, not orthographic (spelling-based) similarity. Two words that are close in spelling but semantically unrelated (e.g., form vs. from) __may not__ be near in the GloVe space. This could limit the system's ability to correct specific errors where semantic context is weak or absent.\n",
    "* __Lexicon Size and Memory Constraints__: A modern Hopfield network stores patterns (in this case, correct word embeddings) as attractors in its memory. To be effective, the network must store a sufficiently large lexicon to cover the domain of interest. However, as the lexicon grows, the network's memory requirements and retrieval complexity increase, which may pose practical challenges for large-scale dictionaries.\n",
    "\n",
    "### Can we do phrases, or just single words?\n",
    "Using multihead attention for phrases can help tackle limitations of word-level models by focusing on contextual relationships between words—here’s why this idea is strong and a few things to think about:\n",
    "* __Captures contextual cues__: Single words (especially misspelled ones) can be ambiguous, but phrases or sentences provide rich context. Multihead attention allows our model to focus on different aspects of the phrase (e.g., syntax, semantics), which can disambiguate corrections.\n",
    "*__Deals with word dependencies__: Many spelling errors are clearer when you consider surrounding words (e.g., “Their going to the store” vs “They’re going to the store”). Attention lets your model weigh relevant parts of the phrase to guide correction.\n",
    "\n",
    "However, we must consider extending our single attention mechanism to a multi-head attention mechanism. That sounds like another cool question!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efc360",
   "metadata": {},
   "source": [
    "## Tests\n",
    "In the code block below, we check some values in your notebook and give you feedback on which items are correct or different. `Unhide` the code block below (if you are curious) about how we implemented the tests and what we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5bc4aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:                                                 | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "CHEME 5820 Practicum S2025                                    | \u001b[32m  22  \u001b[39m\u001b[36m   22  \u001b[39m\u001b[0m0.3s\n",
      "  Task 1: Setup, Prerequisites and Data                       | \u001b[32m   8  \u001b[39m\u001b[36m    8  \u001b[39m\u001b[0m0.3s\n",
      "  Task 2: Recovering a word from an uncorrupted memory vector | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m0.0s\n",
      "  Task 3: Recovering a word from a corrupted memory vector    | \u001b[32m   5  \u001b[39m\u001b[36m    5  \u001b[39m\u001b[0m0.0s\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "    @testset verbose = true \"CHEME 5820 Practicum S2025\" begin\n",
    "\n",
    "        @testset \"Task 1: Setup, Prerequisites and Data\" begin\n",
    "            @test _DID_INCLUDE_FILE_GET_CALLED == true\n",
    "            @test isnothing(number_of_words_to_memorize) == false\n",
    "            @test isnothing(number_of_embedding_dimesions) == false\n",
    "            @test isnothing(β) == false\n",
    "            @test isnothing(word2vec) == false\n",
    "            @test length(test_words) == number_of_words_to_memorize;\n",
    "            @test size(test_vocabulary, 1) == number_of_embedding_dimesions;\n",
    "            @test size(test_vocabulary, 2) == number_of_words_to_memorize;\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Recovering a word from an uncorrupted memory vector\" begin\n",
    "            @test isnothing(mymodel) == false\n",
    "            @test size(mymodel.X, 1) == number_of_embedding_dimesions\n",
    "            @test size(mymodel.X, 2) == number_of_words_to_memorize\n",
    "            @test isnothing(sₒ) == false\n",
    "            @test length(sₒ) == number_of_embedding_dimesions\n",
    "            @test isnothing(memoryindextorecover) == false\n",
    "            @test length(fₒ) > 0\n",
    "            @test length(pₒ) > 0\n",
    "            @test did_I_answer_DQ1 == true;\n",
    "        end\n",
    "\n",
    "        @testset \"Task 3: Recovering a word from a corrupted memory vector\" begin\n",
    "            @test isnothing(s₁) == false\n",
    "            @test length(s₁) == number_of_embedding_dimesions\n",
    "            @test length(f₁) > 0\n",
    "            @test length(p₁) > 0\n",
    "            @test did_I_answer_DQ2 == true;\n",
    "        end\n",
    "    end\n",
    "end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
