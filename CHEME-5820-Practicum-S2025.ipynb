{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad71c3a8-bc41-47c2-bfaa-4757cbceedf2",
   "metadata": {},
   "source": [
    "# Practicum: Modern Hopfield Network Spell Checking and Word Recommendation\n",
    "___\n",
    "In this practicum problem, we'll implement a modern Hopfield Network and use it for spell checking and word recommendation tasks.\n",
    "\n",
    "* _What are Hopfield Networks_? Hopfield Networks are used for associative memory, where the network can recall a pattern from a partial input. The modern version of Hopfield Networks uses continuous values instead of binary values, and it can store multiple patterns. \n",
    "* We'll use the following paper to guide our implementation and analysis: [Ramsauer, H., Schafl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., Pavlovi'c, M., Sandve, G.K., Greiff, V., Kreil, D.P., Kopp, M., Klambauer, G., Brandstetter, J., & Hochreiter, S. (2020). Hopfield Networks is All You Need. ArXiv, abs/2008.02217.](https://arxiv.org/abs/2008.02217)\n",
    "\n",
    "## Tasks\n",
    "Before we get started, we'll quickly review modern Hopfied Networks. Then, you'll execute the `Run All Cells` command to check if you (or your neighbor) have any code or setup issues. Code issues, then raise your hands - and let's get those fixed!\n",
    "\n",
    "* __Task 1: Setup, Data, Constants (5 min)__: Let's take 5 minutes to load [a Simpsons character library from Kaggle](https://www.kaggle.com/datasets/kostastokis/simpsons-faces) that our Hopfield network will memorize.\n",
    "*  __Task 2: Build a Modern Network Model (5 min)__: In this task, we'll formulate the image dataset we give the network and then create a model of a modern Hopfield network. We'll also quickly check to ensure we are doing what we think we are doing.\n",
    "* __Task 3: Retrieve a memory from the network (30 min)__: In this task, we will retrieve a memory from the modern Hopfield network starting from a random state vector $\\mathbf{s}_{\\circ}$. We'll corrupt an image (by cutting off some fraction of the image) and then see if the model recovers the correct memory given the corrupted starting point. \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e0a48",
   "metadata": {},
   "source": [
    "## Background\n",
    "A modern Hopfield network addresses many of the perceived limitations of the original Hopfield network. The original Hopfield network was limited to binary values and could only store a limited number of patterns. The modern Hopfield network uses continuous values and can store a large number of patterns.\n",
    "* For a detailed discussion of the key milestones in the development of modern Hopfield networks, check out [Hopfield Networks is All You Need Blog, GitHub.io](https://ml-jku.github.io/hopfield-layers/)\n",
    "\n",
    "### Algorithm\n",
    "The user provides a set of memory vectors $\\mathbf{X} = \\left\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{m}\\right\\}$, where $\\mathbf{x}_{i} \\in \\mathbb{R}^{n}$ is a memory vector of size $n$ and $m$ is the number of memory vectors. Further, the user provides an initial _partial memory_ $\\mathbf{s}_{\\circ} \\in \\mathbb{R}^{n}$, which is a vector of size $n$ that is a partial version of one of the memory vectors and specifies the _temperature_ $\\beta$ of the system.\n",
    "\n",
    "__Initialize__ the network with the memory vectors $\\mathbf{X}$, and the inverse temperature $\\beta$. Set current state to the initial state $\\mathbf{s} \\gets \\mathbf{s}_{\\circ}$\n",
    "\n",
    "Until convergence __do__:\n",
    "   1. Compute the _current_ probability vector defined as $\\mathbf{p} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s})$ where $\\mathbf{s}$ is the _current_ state vector, and $\\mathbf{X}^{\\top}$ is the transpose of the memory matrix $\\mathbf{X}$.\n",
    "   2. Compute the _next_ state vector $\\mathbf{s}^{\\prime} = \\mathbf{X}\\mathbf{p}$ and the _next_ probability vector $\\mathbf{p}^{\\prime} = \\texttt{softmax}(\\beta\\cdot\\mathbf{X}^{\\top}\\mathbf{s}^{\\prime})$.\n",
    "   3. If $\\mathbf{p}^{\\prime}$ is _close_ to $\\mathbf{p}$ or we run out of iterations, then __stop__. For example, $\\lVert \\mathbf{p}^{\\prime} - \\mathbf{p}\\rVert_{2}^{2} \\leq \\epsilon$ for some small $\\epsilon > 0$.\n",
    "   4. Otherwise, update the state $\\mathbf{s} \\gets\\mathbf{s}^{\\prime}$, and __go back to__ step 1.\n",
    "\n",
    "   \n",
    "This algorithm is implemented in [the `recover(...)` method](src/Compute.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b935753",
   "metadata": {},
   "source": [
    "## Task 1: Setup, Data, and Prerequisites\n",
    "We set up the computational environment by including the `Include.jl` file, loading any needed resources, such as sample datasets, and setting up any required constants. \n",
    "* The `Include.jl` file also loads external packages, various functions that we will use in the exercise, and custom types to model the components of our problem. It checks for a `Manifest.toml` file; if it finds one, packages are loaded. Other packages are downloaded and then loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184b2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\"); # load a bunch of libs, including the ones we need to work with images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aba20e",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Before we load the data, let's set up some constants that we will use in the exercise. \n",
    "\n",
    "__TODO__: Please enter the number of words (memories) you want your Hopefield network to memorize, the embedding dimension and the inverse temperature $\\beta$.\n",
    "\n",
    "* The `number_of_words_to_memorize::Int` should be less than or equal to the number of words in the dataset that we load below.\n",
    "* The `number_of_embedding_dimesions::Int` depends upon the pretrained embedding dataset that we will use. In this case, we will use the [GloVe](https://nlp.stanford.edu/projects/glove/) dataset, with `50` embedding dimensions.\n",
    "* The inverse temperature $\\beta > 0$ is a hyperparameter that controls the sharpness of the softmax distribution. A higher value of $\\beta$ will make the distribution sharper, while a lower value will make it smoother. Set an initial value of `0.5` for $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f255b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words_to_memorize = 2^5; # TODO: Enter how many mempories we want to memorize (Int ≥ 0)\n",
    "number_of_embedding_dimesions = 50; # TODO: Enter the number of embedding dimensions for each word (Int = 50)\n",
    "β = 1.5; # TODO: Enter an inverse temperature of the system (high T -> low β) Float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cc6c7",
   "metadata": {},
   "source": [
    "### Data\n",
    "In this section, let's load a vector embedding from words. We'll use the [GloVe pretrained word embedding dataset](https://nlp.stanford.edu/projects/glove/) to as the memories for our Hopfield model. The **GloVe (Global Vectors for Word Representation)** dataset is a widely used pre-trained word embedding resource developed by Pennington, Socher, and Manning at Stanford University. \n",
    "* _What is it?_ It constructs vector representations of words by aggregating global word co-occurrence statistics from a corpus, enabling semantic relationships to be captured in vector space. GloVe embeddings have been trained on large datasets such as Wikipedia, Gigaword, and Common Crawl, offering dimensionalities typically ranging from 50 to 300. These embeddings are foundational for many NLP tasks, including text classification, sentiment analysis, and machine translation.\n",
    "* See: [Pennington et al., EMNLP 2014, GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) for more details on this dataset.\n",
    "\n",
    "This dataset is large, so we won't check it into the repository. Instead, we'll download it from the internet. Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34627ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec, vec2word = let\n",
    "\n",
    "    # do we have the embeddings downloaded?\n",
    "    data = nothing;\n",
    "    if (isfile(joinpath(_PATH_TO_DATA, \"glove_6B_50d.jld2\")) == false) \n",
    "        # TODO: download logic goes here ...\n",
    "    else\n",
    "        data = JLD2.load(joinpath(_PATH_TO_DATA, \"glove_6B_50d.jld2\")) # Ok, we have the embeddings file, so let's load it\n",
    "    end\n",
    "\n",
    "    # load the embeddings\n",
    "    word2vec = data[\"word2vec\"] # this is a Dict{String, Tuple{Float32}}\n",
    "    vec2word = data[\"vec2word\"] # this is a Dict{Tuple{Float32}, String}\n",
    "\n",
    "    # return -\n",
    "    (word2vec, vec2word)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221159d",
   "metadata": {},
   "source": [
    "__Test data__: Now that we have loaded the pretrained GloVe dataset, let's select a (random) subset of length `number_of_words_to_memorize` from the dataset to encode into the modern Hopfield network. \n",
    "* This code block returns the `test_words::Array{String,1}` array of words that we will use to test the Hopfield network. The embedding vectors corresponding to these words are stored in the `test_vocabulary::Array{Float64,2}` array, where the vector representation of each word is stored in the columns of this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "99c94374",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words, test_vocabulary = let\n",
    "\n",
    "    # initialize -\n",
    "    vocabulary = Array{Float64,2}(undef, number_of_words_to_memorize, number_of_embedding_dimesions); # this is a matrix of Float64\n",
    "    test_words = Array{String,1}(undef, number_of_words_to_memorize); # this is a vector of strings\n",
    "    total_number_of_words = length(word2vec); # this is the total number of words in the dataset\n",
    "    index_of_words_to_learn = randperm(total_number_of_words)[1:number_of_words_to_memorize]; # this is the random index of words to learn\n",
    "\n",
    "    # get the keys of word2vec -\n",
    "    words = keys(word2vec) |> collect; # this is a vector of strings\n",
    "\n",
    "    # loop over the words to learn\n",
    "    for i ∈ eachindex(index_of_words_to_learn)\n",
    "        \n",
    "        j = index_of_words_to_learn[i]; # this is the index of the word we want to learn\n",
    "        wⱼ = words[j]; # this is the word we want to learn\n",
    "        test_words[i] = wⱼ; # this is the word we want to learn \n",
    "        embedding = word2vec[wⱼ]; # this is the embedding of the word we want to learn\n",
    "\n",
    "        for j ∈ 1:number_of_embedding_dimesions\n",
    "            vocabulary[i,j] = embedding[j]; # this is the embedding of the word we want to learn\n",
    "        end\n",
    "    end\n",
    "\n",
    "    test_vocabulary = vocabulary |> transpose |> Matrix; # this is the vocabulary we want to learn\n",
    "\n",
    "    # return -\n",
    "    (test_words, test_vocabulary);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8722efc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Vector{String}:\n",
       " \"mckone\"\n",
       " \"matraville\"\n",
       " \"signallers\"\n",
       " \"scei\"\n",
       " \"belisle\"\n",
       " \"subservient\"\n",
       " \"infallible\"\n",
       " \"agaricales\"\n",
       " \"hillmon\"\n",
       " \"coptic\"\n",
       " ⋮\n",
       " \"sagd\"\n",
       " \"50.17\"\n",
       " \"crimen\"\n",
       " \"videanu\"\n",
       " \"frisco\"\n",
       " \"chot\"\n",
       " \"neufville\"\n",
       " \"jains\"\n",
       " \"maldacena\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa383c44",
   "metadata": {},
   "source": [
    "__Check__: Let's check that the `test_words` and `test_vocabulary` arrays are of the correct size. \n",
    "* The `test_words` array should be of size `number_of_words_to_memorize`. The `test_vocabulary` array should be of size `number_of_embedding_dimensions` $\\times$ `number_of_words_to_memorize`, i.e., the memorized words should be on the columns of the `test_vocabulary` array.\n",
    "* We'll use the [@assert macro](hhttps://docs.julialang.org/en/v1/base/base/#Base.@assert) to check that the arrays are of the correct size. If the assertion fails, an error will be raised and the program will stop. If no error is raised, the program will continue (everything is correctly sized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc729cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    @assert length(test_words) == number_of_words_to_memorize;\n",
    "    @assert size(test_vocabulary, 1) == number_of_embedding_dimesions;\n",
    "    @assert size(test_vocabulary, 2) == number_of_words_to_memorize;\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ddd93",
   "metadata": {},
   "source": [
    "## Task 2: Can we recover an uncorrupted memory?\n",
    "In this task, we'll create a modern Hopfield network model, load the test data, and then check if we can recover an _uncoruppted_ memory from the network. We'll do this by starting from a state vector $\\mathbf{s}_{\\circ}$ that is a column from the data loaded into the model.\n",
    "\n",
    "* __Convergence__: We are _guaranteed_ that the network will converge to a local minimum, but we are not guaranteed that the local minimum corresponds to the the original _uncoruppted_ memory.\n",
    "* __Expectation__: We expect to recover the original _uncoruppted_ memory with a small number of mistakes when the system temperature is cold, i.e., $\\beta > \\beta^{\\star}$, where $\\beta^{\\star}$ is a (unknown) threshold temperature that (potentially) depends on the number of memories we are memorizing. We'll play around with the inverse temperature below.\n",
    " \n",
    "\n",
    "Let's start by creating a model of a modern Hopfield network. \n",
    "* We'll construct [a `MyModernHopfieldNetworkModel` instance](src/Types.jl) using a custom [`build(...)` function](src/Factory.jl). The [`build(...)` method](src/Factory.jl) takes the type of thing we want to build, the (linearized) image library we want to encode, and the (inverse) system temperature $\\beta$ as inputs — images along the columns.\n",
    "* The [`build(...)` function](src/Factory.jl) returns a `MyModernHopfieldNetworkModel` instance, where the image library is stored in the `X::Array{Float64,2}` field, and the system temperature is stored in the `β::Float64` field.\n",
    "\n",
    "We'll store the Hopfield network instance in the `mymodel::MyModernHopfieldNetworkModel` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0b88f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = let\n",
    "\n",
    "    # initialize -\n",
    "    model = nothing; # this is the model we want to build\n",
    "    memorycollection =test_vocabulary; # words (memories) on columns\n",
    "    index_vector = 1:number_of_words_to_memorize |> collect; # this is the index of the words we want to learn\n",
    "    words = keys(test_vocabulary) |> collect; # this is a vector of strings\n",
    "    \n",
    "    # build model -\n",
    "    model = build(MyModernHopfieldNetworkModel, (\n",
    "            memories = memorycollection, # this is the data we want to memorize. Images on columns\n",
    "            β = β, # Inverse temperature of the system. A big beta means we are more likely to get the right answer\n",
    "    ));\n",
    "\n",
    "    model; # return the model to the calling scope\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b18b75",
   "metadata": {},
   "source": [
    "__Check__: Let's do a quick check to make sure we are doing what we think we are doing when we loaded the memories into the model. The columns of the `model.X` field should be the words that we are encoding into the Hopfield network. Thus, we should be able to grab a column from `model.X` and look it up in the original `vec2word` dictionary.\n",
    "* We'll use the [@assert macro](hhttps://docs.julialang.org/en/v1/base/base/#Base.@assert) to check that the true word, and the word encoded in the model are the same. If the assertion fails, an error will be raised and the program will stop. If no error is raised, the program will continue (everything is correct)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8905f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    \n",
    "    # initialize -\n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    index_to_check = rand(1:number_of_words_to_memorize); # what index do we want to check? (random)\n",
    "    \n",
    "    # Get the true word, and the word we think we learned -\n",
    "    eᵢ = X[:,index_to_check] |> Tuple # this is the embedding of the word we want to learn\n",
    "    wᵢ = test_words[index_to_check]; # this is the word we want to learn\n",
    "    ŵᵢ = vec2word[eᵢ]; # this is the word we think we learned\n",
    "\n",
    "    # Compare the two words -\n",
    "    @assert wᵢ == ŵᵢ; # this is the word we want to learn\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56cf809",
   "metadata": {},
   "source": [
    "__Retrieve a memory from the network__: Next, we'll test if we can recover uncorrupted and corrupted memories from the Hopfield network.\n",
    "Let's start by specifying which memory we are trying to recover in the `memoryindextorecover::Int` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e93d68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "memoryindextorecover = 21; # TODO: Specify which memory vector will we choose (must be between 1 and number_of_words_to_memorize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193276",
   "metadata": {},
   "source": [
    "Next, let's build an uncorrupted and corrupted initial condition vector using the true word emebedding vector. We'll store \n",
    "the uncorrupted word in the `sₒ::Array{Float64,1}` variable, while the corrupted word will be stored in the `s₁::Array{Float64,1}` variable. Let's start with the uncorrupted memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5184ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sₒ = mymodel.X[:,memoryindextorecover]; # this is the memory vector we want to recover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92773242",
   "metadata": {},
   "source": [
    "What word does `memoryindextorecover::Int64` point to? Fill me in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac7067c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word at index 21 that we (think) we encoded is: verkin. Check: the true word is: verkin\n"
     ]
    }
   ],
   "source": [
    "let \n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    p = β*(transpose(X) * sₒ) |> s-> NNlib.softmax(s) # this is the probability of the word we want to learn\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    w = test_words[memoryindextorecover]; # this is the word we want to learn\n",
    "    println(\"The word at index $(memoryindextorecover) that we (think) we encoded is: $(ŵ). Check: the true word is: \", w);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2558716a",
   "metadata": {},
   "source": [
    "Now that we have a starting memory encoded in the state vector $\\mathbf{s}_{\\circ}$, can we recover the original uncorrupted word? We are guaranteed a word, but maybe _not_ the correct one.\n",
    "* _Implementation_: We implemented the modern Hopfield recovery algorithm above in [the `recover(...)` method](src/Compute.jl). This method takes our `model::MyModernHopfieldNetworkModel` instance, the initial configuration vector `sₒ::Array{Float64,1}`, the maximum number `maxiterations::Int64`, and an iteration tolerance parameter `ϵ::Float64`. This method will continue to iterate until the probability vector converges, or we run out of iterations.\n",
    "* [The `recover(...)` method](src/Compute.jl) returns the recovered word vector in the `ŝₒ::Array{Float32,1}` variable, the word vectors at each iteration are stored in the `fₒ::Dict{Int, Array{Float64,2}}` dictionary, and the probability of the words at each iteration in the `pₒ::Dict{Int, Array{Float64,2}}` variable. The dictionaries are indexed from `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3049a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ŝₒ,fₒ,pₒ) = recover(mymodel, sₒ, maxiterations = 10000, ϵ = 1e-16); # iterate until we hit stop condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b18c41",
   "metadata": {},
   "source": [
    "How many iterations did it take to converge? (this will be the length, i.e.,. the numbr of keys of the `fₒ` dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d6ef339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many iterations: 3\n"
     ]
    }
   ],
   "source": [
    "println(\"How many iterations: $(length(fₒ))\") # how many iterations did we need to converge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a404ed",
   "metadata": {},
   "source": [
    "__Which word did we recover?__ We can check if the recovered word is what we expected by looking at the probability of the recovered words stored in the `pₒ::Dict{Int, Array{Float64,2}}` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29818778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"verkin\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recovered_word_uncorrupted = let \n",
    "    \n",
    "    # initialize -\n",
    "    number_of_iterations = length(fₒ); # how many iterations did we need to converge? \n",
    "    p = pₒ[number_of_iterations - 1]; # this is the probability of the word we want to learn  (0 based)\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    \n",
    "    ŵ; # return the word we *think* we learned\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8309f2",
   "metadata": {},
   "source": [
    "__Check__: Let's check to see if the recovered word is identical to the original word (not guaranteed). We can do this by checking the `s₁::Array{Float32,1}` variable against the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "13cbb970",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    true_word = test_words[memoryindextorecover]; # this is the word we want to learn\n",
    "    @assert recovered_word_uncorrupted == true_word\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cdd8dd",
   "metadata": {},
   "source": [
    "## Task 3: Retrieve a corrupted memory from the network\n",
    "In this task, we'll repeat the process above, but this time we'll start from a corrupted memory. We'll do this by cutting off a fraction of the word and then see if the model recovers the correct memory given the corrupted starting point. \n",
    "\n",
    "Let's get started by building a corrupted memory. We'll iterate through each embedding dimension from the uncorrupted word; sometimes, we'll make a mistake and replace the correct embedding value with an incorrect value. We'll control how oftern we make a mistake using the hyperparameter $\\theta$.\n",
    "* _What is the $\\theta$ parameter?_ The $\\theta$ hyperparameter controls how often we make mistakes. Its interpretation depends upon our _mistake_ model. For example, if we are cutting off some fraction of the embedding dimension, then $\\theta$ describes the fraction of the image we are cutting off. \n",
    "\n",
    "Whichever mistake model we use, the $\\theta\\in[0,1]$. We store the corrupted word in the `s₁::Array{Float64,1}` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b286495",
   "metadata": {},
   "outputs": [],
   "source": [
    "s₁ = let\n",
    "\n",
    "    # initialize -\n",
    "    sₒ = mymodel.X[:,memoryindextorecover]; # this is the memory vector we want to recover (uncorrupted)\n",
    "    s₁ = Array{Float32,1}(undef, number_of_embedding_dimesions); # initialize some space to store the corrupted word\n",
    "    θ = 0.4; # TODO: set a mistake threshold (1 - θ is the fraction the original memory that we retain)\n",
    "\n",
    "    # Corruption model: Cutoff part of the memory\n",
    "    cutoff = (1-θ)*number_of_embedding_dimesions |> x-> round(Int,x);\n",
    "    for i ∈ 1:number_of_embedding_dimesions\n",
    "        eᵢ =  sₒ[i]; # We have some gray-scale values in the original vector, need to perturb\n",
    "        if (i ≤ cutoff)\n",
    "            s₁[i] = eᵢ;\n",
    "        else\n",
    "            s₁[i] = β*randn(); # add some random noise (proportional to β)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    s₁ # return corrupted data to the calling scope\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455bfb7",
   "metadata": {},
   "source": [
    "__What is the closet word to the corrupted word__? Using the inner product self attention mechanism as our measure of similarity, we can compute the most probable memory given the corrupted memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6567f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= ======= ==============\n",
      " \u001b[1m       word \u001b[0m \u001b[1m index \u001b[0m \u001b[1m probability \u001b[0m\n",
      " \u001b[90m     String \u001b[0m \u001b[90m Int64 \u001b[0m \u001b[90m     Float64 \u001b[0m\n",
      "============= ======= ==============\n",
      "      verkin      21      0.998894\n",
      "  agaricales       8   0.000455889\n",
      "  infallible       7   0.000308335\n",
      "      crimen      26    0.00020042\n",
      "   neufville      30    9.22495e-5\n",
      "============= ======= ==============\n"
     ]
    }
   ],
   "source": [
    "let\n",
    "    # initialize -\n",
    "    number_of_top_words = 5; # TODO: You can see how many words are the closest to the corrupted memory by changing this number\n",
    "    X = mymodel.X; # get the training data in the model\n",
    "    p = β*(transpose(X) * s₁) |> s-> NNlib.softmax(s) # this is the probability of the word we want to learn\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "\n",
    "    # make a table -\n",
    "    df = DataFrame();\n",
    "    sorted_indices = sortperm(p, rev=true); # sort the indices of the probabilities\n",
    "    for i ∈ 1:number_of_top_words\n",
    "        index = sorted_indices[i]; # this is the index of the word we think we learned\n",
    "        ŵ = test_words[index]; # this is the word we think we learned\n",
    "        p̂ = p[index]; # this is the probability of the word we think we learned\n",
    "        push!(df, (word=ŵ, index = index, probability=p̂)); # add the word and its probability to the table\n",
    "    end\n",
    "    pretty_table(df, tf = tf_simple)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18945b96",
   "metadata": {},
   "source": [
    "__Do we converge to the correct word starting from corrupted word__? Now that we have a starting (corrupted) memory encoded in the $\\mathbf{s}_{1}$ state vector, can we recover the original uncorrupted memory, i.e., the uncorrputed word? We are guaranteed to converge to a word, but maybe _not_ the correct one.\n",
    "* _Implementation_: We implemented the modern Hopfield recovery algorithm above in [the `recover(...)` method](src/Compute.jl). This method takes our `model::MyModernHopfieldNetworkModel` instance, the initial configuration vector `sₒ::Array{Int32,1}`, and the maximum number `maxiterations::Int64`, and iteration tolerance parameter `ϵ::Float64`. \n",
    "* [The `recover(...)` method](src/Compute.jl) returns the recovered image in the e`s₁::Array{Float32,1}` variable, the image at each iteration in the `f::Dict{Int, Array{Float32,2}}` dictionary, and the probability of the image at each iteration in the `p::Dict{Int, Array{Float32,2}}` variable. The frames and probability dictionaries are indexed from `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99768f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ŝ₁,f₁,p₁) = recover(mymodel, s₁, maxiterations = 10000, ϵ = 1e-16); # iterate until we hit stop condition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d370391",
   "metadata": {},
   "source": [
    "How many iterations did it take to converge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dac18af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network converged to an answer (staring from a corrupted word) in: 5 iterations.\n"
     ]
    }
   ],
   "source": [
    "println(\"The network converged to an answer (staring from a corrupted word) in: $(length(f₁)) iterations.\") # how many iterations did we need to converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f4ee0b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"verkin\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recovered_word_corrupted = let \n",
    "    \n",
    "    # initialize -\n",
    "    number_of_iterations = length(f₁); # how many iterations did we need to converge? \n",
    "    p = p₁[number_of_iterations - 1]; # this is the probability of the word we want to learn  (0 based)\n",
    "    ŵ = argmax(p) |> i-> test_words[i]; # this is the index of the word we think we learned\n",
    "    \n",
    "    ŵ; # return the word we *think* we recovered\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efc360",
   "metadata": {},
   "source": [
    "## Tests\n",
    "In the code block below, we check some values in your notebook and give you feedback on which items are correct or different. `Unhide` the code block below (if you are curious) about how we implemented the tests and what we are testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5bc4aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:                                              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "CHEME 5820 Practicum S2025                                 | \u001b[32m  20  \u001b[39m\u001b[36m   20  \u001b[39m\u001b[0m0.0s\n",
      "  Task 1: Setup, Prerequisites and Data                    | \u001b[32m   8  \u001b[39m\u001b[36m    8  \u001b[39m\u001b[0m0.0s\n",
      "  Task 2: Recovering a word from a memory vector           | \u001b[32m   8  \u001b[39m\u001b[36m    8  \u001b[39m\u001b[0m0.0s\n",
      "  Task 3: Recovering a word from a corrupted memory vector | \u001b[32m   4  \u001b[39m\u001b[36m    4  \u001b[39m\u001b[0m0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"CHEME 5820 Practicum S2025\", Any[Test.DefaultTestSet(\"Task 1: Setup, Prerequisites and Data\", Any[], 8, false, false, true, 1.746362929225082e9, 1.746362929225126e9, false, \"/Users/jeffreyvarner/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Practicum-S2025/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl\"), Test.DefaultTestSet(\"Task 2: Recovering a word from a memory vector\", Any[], 8, false, false, true, 1.746362929225135e9, 1.746362929225167e9, false, \"/Users/jeffreyvarner/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Practicum-S2025/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl\"), Test.DefaultTestSet(\"Task 3: Recovering a word from a corrupted memory vector\", Any[], 4, false, false, true, 1.746362929225174e9, 1.746362929225189e9, false, \"/Users/jeffreyvarner/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Practicum-S2025/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl\")], 0, false, true, true, 1.746362929225051e9, 1.746362929225192e9, false, \"/Users/jeffreyvarner/Desktop/julia_work/CHEME-5820-SP25/CHEME-5820-Practicum-S2025/jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X65sZmlsZQ==.jl\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "let \n",
    "    @testset verbose = true \"CHEME 5820 Practicum S2025\" begin\n",
    "\n",
    "        @testset \"Task 1: Setup, Prerequisites and Data\" begin\n",
    "            @test _DID_INCLUDE_FILE_GET_CALLED == true\n",
    "            @test isnothing(number_of_words_to_memorize) == false\n",
    "            @test isnothing(number_of_embedding_dimesions) == false\n",
    "            @test isnothing(β) == false\n",
    "            @test isnothing(word2vec) == false\n",
    "            @test length(test_words) == number_of_words_to_memorize;\n",
    "            @test size(test_vocabulary, 1) == number_of_embedding_dimesions;\n",
    "            @test size(test_vocabulary, 2) == number_of_words_to_memorize;\n",
    "        end\n",
    "\n",
    "        @testset \"Task 2: Recovering a word from a memory vector\" begin\n",
    "            @test isnothing(mymodel) == false\n",
    "            @test size(mymodel.X, 1) == number_of_embedding_dimesions\n",
    "            @test size(mymodel.X, 2) == number_of_words_to_memorize\n",
    "            @test isnothing(sₒ) == false\n",
    "            @test length(sₒ) == number_of_embedding_dimesions\n",
    "            @test isnothing(memoryindextorecover) == false\n",
    "            @test length(fₒ) > 0\n",
    "            @test length(pₒ) > 0\n",
    "        end\n",
    "\n",
    "        @testset \"Task 3: Recovering a word from a corrupted memory vector\" begin\n",
    "            @test isnothing(s₁) == false\n",
    "            @test length(s₁) == number_of_embedding_dimesions\n",
    "            @test length(f₁) > 0\n",
    "            @test length(p₁) > 0\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
